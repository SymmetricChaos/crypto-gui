{
  "ASCII": "The American Standard Code for Information Interchange (ASCII) was created in the 1960s and consists of 128 characters sufficient to write essentially all English text and to control the printing device. Originally a 7-bit code it is now most often seen as an 8-bit code with the leading bit always set to zero. The limitations of ASCII became more apparent as it emerged as a defacto standard for computer text even outside the United States. Today the UTF-8 of Unicode encoding fully subsumes ASCII.\nTher are 34 ASCII control codes that do not print a visible character. These incldue the null character (NUL) and the space character (SP). When encoding this page accepts both literal control characters and their control pictures. When decoding the control pictures are always used except for the SPACE control picture.",
  "Ascii85": "Ascii85 is binary-to-text encoding that uses 85 ASCII characters. It works by taking a 32 bit chunk of data and dividing it by 85 until zero is reached, keeping track of remainders. The remainders are then encoded as ASCII characters.",
  "Bacon": "Fancis Bacon developed this fixed width code (though he called it a cipher) in 1605, assigning each letter of the classical Latin alphabet a five bit code. The intended use was to hide one text in another by writing letters of the larger text in two different ways to represent the bits of the hidden message.",
  "Base32": "Base32 works similarly to the more popular Base64 encoding but operates on groups of five bits. This introduces more overhead than Base64 but the smaller character set makes it possible to remove characters that might be confused, important if the string must be copied by a person, or even to make forming words impossible, which may be a requirement if stored on a system that forbids profanity or other kinds of text.",
  "Base64": "Base64 is a popular form of binary-to-text encoding that converts arbitary data into a sequence of characters. The original goal of this was to ensure that systems that handled bytes of data in different ways but all recognized certain characters could handle the data without corrupting it. Today it is more common as a method to obscure the data. Each possible six bit group is encoded as an eight bit ASCII character for about 33% overhead. The input on this site only allowed UTF-8 encoded text strings.",
  "Baudot": "The Baudot Code is a five bit telegraphy code developed by Emile Baudot of France that extended its character set by being able to switch modes. It was succeeded by a variant, called ITA2, created by Donald Murray of New Zealand would become the major international standard for text communication until the rise of ASCII. The data rate measure of symbols per second is known as the 'baud' after this code.\n\nAn important part of the Baudot codes is control characters. These are represented here by Unicode control pictures.\n␀: The null character, nothing transmitted.\n␍: Carriage Return, place the printing head at the start of a line.\n␊: Line Feed, move to the next line\n␎: Figure Shift (Shift Out), change to the number and symbols mode\n␏: Letter Shift (Shift In), change to the letter mode\n␅: Enqiry, test for an automatic response from the receiving station\n␇: Bell, ring a bell at the receiving station",
  "Block": "In a block code (or fixed-width code) every code group contains the same number of symbols. The ASCII, UTF-32, and Bacon codes are examples of block codes. All block codes are examples of prefix codes.",
  "Elias": "The family of Elias codes are prefix codes.",
  "Fibonacci": "The Fibonacci code is a prefix code that assigns a binary sequence to each positive integer. So long as larger integers appear less frequently than smaller ones this encoding is within a constant factor of an optimal encoding. More importantly the Fibonacci code is self-synchronizing. Alterting a single bit will either incorrectly split a single code or incorrectly merge two codes but following codes will not be affected.",
  "Gödel": "Gödel's encoding was designed to allow statements of mathematical logic to be encoded as numbers so that mathematical logic could be applied to itself. The technique is simple: A positive integer is assigned to each word or symbol, then a finite sequence of integers is encoded as the sequence of primes raised to the power of each value and that is multiplied together. The original sequence is recovered by factoring. For example: the sequence of numbers (1, 2, 3) would be encoded as 2^1 × 3^2 × 5^3 = 2 × 9 × 125 = 2250.",
  "Hamming Code": "The family of Hamming Codes are widely used for detecting and correcting errors. They detect two bit errors and automatically correct single bit errors. Their common usage comes from their optimal efficiency for their capability.",
  "ISBN": "The International Standard Book Number (ISBN) is a standard identifier assigned to each edition and variant of a registered publication. From 1953 to 2007 the standard was ten digits until it was updated to match the 13-digit EAN product standard. To catch simple transcrption errors a check digit is calculated and included at the end.",
  "Levenshtein": "The Levenshtein coding is a prefix code that assigns a binary sequence to each non-negative intger.",
  "Linotype": "The internal sorting mechanism of the Linotype machine used a seven-bit code to mechnically identify the characters. Each matrix, a small brass unit with an inscribed character, has seven teeth which engage with seven rails in the distributor system. Gaps in the rails at each channel were the complement of the teeth on the matrix for that letter. This automatic reloading ability, enabled by the code, was a major factor in the machine's success. Note that ligatures may not render correctly on all systems.\n\nFor ease of use on modern computers the ordinary space character may be entered in place of the em-space (0100010) when encoding.",
  "Luhn's Algorithm": "Luhn's Algorithm is a technique for calculating a checksum developed by Hans Luhn in 1960. Digits are taken right to left, the value of every even digit doubled, and then all summed together. That check digit is the smallest number, if added to the total, would make a multiple of 10 (or whatever modulus is chosen).",
  "M-of-N": "An M-of-N code is an error detecting code detects single bit errors but cannot correct errors. Each code word has a specified length (number of bits) and weight (number of bits set to 1). Binary data is re-encoded by reading (length - weight) bits then appending 1 bits until the require weight it reached and then 0 bits until the required length is reached. When decoding a code block with the wrong weight is detected as an error. Here a decoding error is replaced by the � symbol.",
  "Morse": "The best known version of Morse Code is the ITU Standard. It uses two kind of signals the 'dit' and 'dah' with the dah defined as three times the length of the dit. Morse code also requires periods with no signal, called spaces, in order to differentiate characters. The subset of ITU Morse below covers all the printing characters. Additional control signals and prosigns are not yet supported. The space between dits and dahs is the same length as a dit, between characters is a space of three dits. The older American Morse standard is slightly different, still using dits and dahs of the same size but also including a double length space within come characters.",
  "Needle": "The needle code is the coding method using by the earliest electrical telegraph system created by Cooke and Wheatstone. Letters were arranged in a diamond shaped grid with five needles across the middle. Letters were indicated by turning two of the needles so that they pointed to a specific letter. Although it required no training to use the complexity and unreliability of this system was a problem. Later needle codes were created that used just one or two needles and did not operate by pointing.",
  "Parity Bit": "A parity bit is a very simple way of efficiently detecting single bit errors. The bits of some segment are added up and a bit is appended indicating if the value is odd or even. Equivalently the bits are XORed together and the value is appended. In this example the check bit is added immediately after the block though it can be placed anywhere.",
  "PGP Words": "The PGP Words converts a sequence of bytes into a sequence of words. To prevent errors this is done using two list which alternate back and forth. The even list uses words with two syllables and the odd list uses words with three syllables.",
  "Punycode": "Punycode is a method for re-encoding short Unicode strings using only ASCII characters, originally created for use with Internationalized Domain Names. The characters which are not ASCII are stripped out of the string, a delimeter character is placed after the remaining characters, then the non-ASCII characters are encoded onto the end using a method that records their position and Unicode codepoint. For example the sentence \"TạisaohọkhôngthểchỉnóitiếngViệt\" is encoded as \"TisaohkhngthchnitingVit-kjcr8268qyxafd2f1b9g\".",
  "Repetition": "The simplest form of error correcting code is Repetition in which every bit is encoded as that bit repeated several times. When decoding each bit in the block 'votes' for  what the block should decode as with the majority winning. Only odd numbered block sizes are always unambigious. This form of encoding is extremely space inefficient and more sophisticated codes are often used in practice.",
  "Romaji": "It is often convient or necessary for Japanese text to be rendered in Latin characters. To accomplish this an encoding known as romaji (roman characters) is used. Several version are available: Kunrei-shiki is endorsed by the government of Japan, Hebern-shiki is widely used in the West, Nihon-shiki is a highly regular encoding no longer in common use.\nNotes on decoding: Decoding romaji back to kana is an imperfect process. The system used here does not vary with the variant chosen and should provide a decoding of any valid romaji. Because information is lost when encoding this decoding may not be perfectly accurate. For instance the を (wo) and お (o) kana are both transliterated as 'o' in Kunrei and Hebern. Likewise っに and んに are both rendered as `nni` in all encodings used. Decoding is consistent, so 'o' always becomes お while 'nni' always becomes んに but these many not reflect the original kana.",
  "S\/KEY": "The S\/KEY standard covers a system for human readable use of one time passwords. Here is recreated just the binary-to-text encoding used for the 64-bit passwords. Each password is converted into six words representing 11 bits each for a total of 66 bits, the bits of the password followed by a two bit error check.",
  "Spelling Alphabet": "Spelling Alphabets are designed to spell short sequences of letter over voice channels where noise might cause ambiguity. Written specifications for these systems appeared in the early 20th century with several focusing only on the most easily confused letters.",
  "Tap": "Tap codes work on the same principle as the Polybius square cipher but simplified for ease of use. Characters are specified by pairs of tap sequences. The first sequence of taps selects a row and the second sequence of taps a column in that row. This very slow way of encoding text is said to be common among prisoners as it is easy to learn and requires no special tools.",
  "Unary": "The Unary Encoding is the simplest prefix code (meaning no code appears as the prefix of any other) and thus the simplest useful variable length code. Unary codes are extremely inefficient in practice.",
  "Unicode": "Unicode is the dominant international standard for encoding of text using in most of the world's writing systems with over 100,000 code points defined. There are three major encodings used called UTF-8, UTF-16, and UTF-32.",
  "UPC": "The Universal Product Code is a widely used encoding for barcodes. It has twelve digits, including the check digit. It is divided into two sections. The six left digits use a seven bit code while the six right digits use the bitwise inverse of that the same code. The start and end are padded with a guard pattern while the left and right are separated by another pattern.",
  "Verhoeff": "The Verhoeff algorithm was the first checksum algorithm using only decimal digits able to both detect all single digit errors and all adjacent transposition errors. Rather than using standard arithmetic operations calculation is done using a dihedrial group and permutation."
}